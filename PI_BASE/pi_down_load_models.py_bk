from huggingface_hub.hub_mixin import hf_hub_download
from torch.nn.parallel.data_parallel import data_parallel
from modelscope import snapshot_download as ms_snapshot_download
from modelscope.hub.constants import MODEL_ID_SEPARATOR
from huggingface_hub import snapshot_download as hf_snapshot_download

from langchain_ollama import OllamaEmbeddings, OllamaLLM
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os

# å¯¼å…¥PIé¡¹ç›®æ—¥å¿—æ¨¡å—
from pi_log import *

class PI_LLM:
    def __init__(self, isLocal:bool = False):# æ˜¯å¦æœ¬åœ°æ¨¡å‹, é»˜è®¤ä½¿ç”¨è¿œç¨‹æ¨¡å‹
        self.isLocal = isLocal
        self.model_id = None        # æ¨¡å‹IDï¼Œç”¨äºæ ‡è¯†æ¨¡å‹
        self.tokenizer = None       # æœ¬åœ°llmçš„åˆ†è¯å™¨
        self.model = None           # æœ¬åœ°llmçš„æ¨¡å‹
        self.ollama_llm = None      # ollama llm
        self.vllm_llm = None        # vllm llm

    def download_model(self, model_tag, model_dir, model_repo=1):
        """ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹çš„ä¸€ç«™å¼æ–¹æ³•"""
        model_path = download_model_to_local(model_tag, model_dir, model_repo)
        return self.load_model(model_path)

    def load_model(self, model_id):
        if self.isLocal:
            self.tokenizer, self.model = load_llm_from_local(model_id)
        else:
            self.ollama_llm = load_llm_from_ollama(model_id)

'''# å…¨å±€å˜é‡å­˜å‚¨å·²åŠ è½½çš„æ¨¡å‹
_LOADED_MODELS = {}'''

# è·å–loggerå®ä¾‹ï¼ˆå¯ç”¨æ–‡ä»¶æ—¥å¿—ï¼‰
logger = get_logger("PI_LLM", log_to_file=True)

# ä¸‹è½½æ•´ä¸ªæ¨¡å‹ä»“åº“åˆ°æœ¬åœ°ï¼Œ è¿™é‡Œä½¿ç”¨modelscope çš„snapshot_download æ–¹æ³•
# model_repo, å¦‚æœä½¿ç”¨huggingface_hub ä¸‹è½½ï¼Œåˆ™å€¼ä¸º1ï¼Œmodelscope ä¸‹è½½ï¼Œè®¾ç½®MODEL_SOURCE ä¸º2
def download_model_to_local(model_id, model_dir, model_repo):
    """ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç›®å½•"""
    model_path = os.path.join(model_dir, model_id.replace(MODEL_ID_SEPARATOR, "/"))
    
    log_download_start(model_id, model_dir, logger)
    
    if os.path.exists(model_path):
        log_file_exists(model_path, logger)
        return model_path
    else:
        if model_repo == 2:
            try:
                local_dir = ms_snapshot_download(
                    model_id = model_id,
                    revision = "master",
                    cache_dir = model_dir,
                    max_workers = 4
                )
                log_download_complete(local_dir, logger)
                return local_dir
            except Exception as e:
                log_error(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {model_id} - é”™è¯¯: {str(e)}", logger)
                raise
        elif model_repo == 1:
            try:
                local_dir = hf_snapshot_download(
                    repo_id = model_id,
                    revision = "main",
                    cache_dir = model_dir,
                    max_workers = 4
                )
                log_download_complete(local_dir, logger)
                return local_dir
            except Exception as e:
                log_error(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {model_id} - é”™è¯¯: {str(e)}", logger)
                raise
        try:
            local_dir = ms_snapshot_download(
                model_id = model_id,
                revision = "master",
                cache_dir = model_dir,
                max_workers = 4
            )
            log_download_complete(local_dir, logger)
            return local_dir
        except Exception as e:
            log_error(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {model_id} - é”™è¯¯: {str(e)}", logger)
            raise

# ä»æœ¬åœ°çš„ollama åŠ è½½ embeddings æ¨¡å‹
def load_embeddings_model_from_ollama(model_id):
    """ä»æœ¬åœ°åŠ è½½åµŒå…¥æ¨¡å‹"""
    log_info(f"ğŸ“¦ åŠ è½½åµŒå…¥æ¨¡å‹: {model_id}", logger)

    try:
        embeddings = OllamaEmbeddings(
            model = model_id,
            base_url = "http://localhost:11434"  # é»˜è®¤å¯çœç•¥
        )
        log_info(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {model_id}", logger)
        return embeddings
    except Exception as e:
        log_error(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {model_id} - é”™è¯¯: {str(e)}", logger)
        raise

# ä»æœ¬åœ°çš„ollama åŠ è½½ llm æ¨¡å‹
def load_llm_from_ollama(model_id):
    """ä»æœ¬åœ°åŠ è½½ llm æ¨¡å‹"""
    try:
        llm = OllamaLLM(
            model = model_id,
            base_url = "http://localhost:11434"  # é»˜è®¤å¯çœç•¥
        )
        log_info(f"âœ… llm æ¨¡å‹åŠ è½½æˆåŠŸ: {model_id}", logger)
        return llm
    except Exception as e:
        log_error(f"llm æ¨¡å‹åŠ è½½å¤±è´¥: {model_id} - é”™è¯¯: {str(e)}", logger)
        raise   

# åŠ è½½æœ¬åœ° LLMï¼Œåœ¨mac M1èŠ¯ç‰‡ä¸Šä½¿ç”¨ MPS åŠ é€Ÿ
def load_llm_from_local(model_path, use_cache=True):
    """åŠ è½½æœ¬åœ°LLMï¼Œæ”¯æŒç¼“å­˜é¿å…é‡å¤åŠ è½½"""
    log_model_loading_start(model_path, logger)
    
    # æ£€æŸ¥ç¼“å­˜
    if use_cache and model_path in _LOADED_MODELS:
        log_cache_hit(model_path, logger)
        return _LOADED_MODELS[model_path]
    else:
        log_cache_miss(model_path, logger)
        
        # è®¾å¤‡æ£€æµ‹
        if torch.backends.mps.is_available():
            device = "mps"
            data_type = torch.float16
        elif torch.cuda.is_available():
            device = "cuda"
            data_type = torch.float16
        else:
            device = "cpu"
            data_type = torch.float32
            
        log_device_detection(device, str(data_type), logger)

    # åŠ è½½åˆ†è¯å™¨tokenizer å’Œæ¨¡å‹ model
    try:
        # åŠ è½½ tokenizer
        log_info(f"ğŸ”¤ åŠ è½½åˆ†è¯å™¨: {model_path}", logger)
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True
        )

        # åŠ è½½æ¨¡å‹
        log_info(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_path}", logger)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map = device, # "auto"
            trust_remote_code = True,
            dtype = data_type,
        )

        # ç¼“å­˜åˆ†è¯å™¨å’Œæ¨¡å‹ï¼ˆtokenizer, modelï¼‰
        if use_cache:
            _LOADED_MODELS[model_path] = (tokenizer, model)
            log_cache_store(model_path, logger)

        log_model_loading_success(model_path, device, logger)

        # è¿”å› tokenizer å’Œ model
        return tokenizer, model
        
    except Exception as e:
        log_model_loading_failed(model_path, str(e), logger)
        raise

if __name__ == "__main__":
    MODEL_REPO = 2
    MODEL_DIR = "/Users/carlos/Desktop/PileGo.Ai/ahum_llm/llms"
    LLM_MODEL_TAG = "Qwen/Qwen3-0.6B"
    EMBEDDING_MODEL_OLLAMA_TAG = "bge-m3"
    LLM_MODEL_OLLAMA_TAG = "qwen3:8b"
    
    # local_model_dir = download_model_to_local(LLM_MODEL_TAG, MODEL_DIR, MODEL_REPO)

    load_embeddings_model_from_ollama(EMBEDDING_MODEL_OLLAMA_TAG)
    '''
    load_llm_from_ollama(LLM_MODEL_OLLAMA_TAG)
    load_llm_from_local(local_dir, use_cache=False)
    '''
    llm = PI_LLM(isLocal=True)
    local_model_dir = llm.download_model(LLM_MODEL_TAG, MODEL_DIR, MODEL_REPO)
    llm.load_model(local_model_dir)
    print(llm.tokenizer, llm.model)
    