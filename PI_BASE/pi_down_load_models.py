# -*-coding: utf-8 -*-
# LLM/embedding model download, load and run 

from modelscope import snapshot_download as ms_snapshot_download
from modelscope.hub.constants import MODEL_ID_SEPARATOR
from huggingface_hub import snapshot_download as hf_snapshot_download
from langchain_huggingface import HuggingFacePipeline
from langchain_community.embeddings import SentenceTransformerEmbeddings

from langchain_ollama import OllamaEmbeddings, OllamaLLM
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, GenerationConfig

import torch
import os

# å¯¼å…¥PIé¡¹ç›®æ—¥å¿—æ¨¡å—
from pi_log import *

class PiLLM:
    def __init__(self):# æ˜¯å¦æœ¬åœ°æ¨¡å‹, é»˜è®¤ä½¿ç”¨è¿œç¨‹æ¨¡å‹
        self.model_id = None        # æ¨¡å‹IDï¼Œç”¨äºæ ‡è¯†æ¨¡å‹
        self.tokenizer = None       # æœ¬åœ°llmçš„åˆ†è¯å™¨
        self.llm_model = None       # æœ¬åœ°llmçš„æ¨¡å‹
        self.embeddings = None      # embedding æ¨¡å‹

    def download_model(self, model_tag, model_dir, model_repo):
        """ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹çš„ä¸€ç«™å¼æ–¹æ³•"""
        model_path = download_model_to_local(model_tag, model_dir, model_repo)
        return model_path

    def load_llm_model(self, model_id, isLocal = False):
        if isLocal:
            self.tokenizer, self.llm_model = load_llm_from_local(model_id)
        else:
            self.llm_model = load_llm_from_ollama(model_id)

    def load_embeddings_model(self, model_id, isLocal = False):
        if isLocal:
            self.embeddings = load_embeddings_model_from_local(model_id)
        else:
            self.embeddings = load_embeddings_model_from_ollama(model_id)

# è·å–loggerå®ä¾‹ï¼ˆå¯ç”¨æ–‡ä»¶æ—¥å¿—ï¼‰
logger = get_logger("PI_LLM", log_to_file=True)

# ä¸‹è½½æ•´ä¸ªæ¨¡å‹ä»“åº“åˆ°æœ¬åœ°ï¼Œ è¿™é‡Œä½¿ç”¨modelscope çš„snapshot_download æ–¹æ³•
# model_repo, å¦‚æœä½¿ç”¨huggingface_hub ä¸‹è½½ï¼Œåˆ™å€¼ä¸º1ï¼Œmodelscope ä¸‹è½½ï¼Œè®¾ç½®MODEL_SOURCE ä¸º2
def download_model_to_local(model_id, model_dir, model_repo):
    """ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç›®å½•"""
    model_path = os.path.join(model_dir, model_id.replace(MODEL_ID_SEPARATOR, "/"))
    
    log_download_start(model_id, model_dir, logger)
    
    if os.path.exists(model_path):
        log_file_exists(model_path, logger)
        return model_path
    else:
        if model_repo == 2:
            try:
                local_dir = ms_snapshot_download(
                    model_id = model_id,
                    revision = "master",
                    cache_dir = model_dir,
                    max_workers = 4
                )
                log_download_complete(local_dir, logger)
                return local_dir
            except Exception as e:
                log_error(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {model_id} - é”™è¯¯: {str(e)}", logger)
                raise
        elif model_repo == 1:
            try:
                local_dir = hf_snapshot_download(
                    repo_id = model_id,
                    revision = "main",
                    cache_dir = model_dir,
                    max_workers = 4
                )
                log_download_complete(local_dir, logger)
                return local_dir
            except Exception as e:
                log_error(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {model_id} - é”™è¯¯: {str(e)}", logger)
                raise
        try:
            local_dir = ms_snapshot_download(
                model_id = model_id,
                revision = "master",
                cache_dir = model_dir,
                max_workers = 4
            )
            log_download_complete(local_dir, logger)
            return local_dir
        except Exception as e:
            log_error(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {model_id} - é”™è¯¯: {str(e)}", logger)
            raise

# åŠ è½½æœ¬åœ°çš„embedding æ¨¡å‹, ä¸é€šè¿‡ollamaåŠ è½½
def load_embeddings_model_from_local(model_id):
    """ä»æœ¬åœ°åŠ è½½åµŒå…¥æ¨¡å‹"""
    log_info(f"ğŸ“¦ åŠ è½½åµŒå…¥æ¨¡å‹: {model_id}", logger)
    
    try:
        embeddings = SentenceTransformerEmbeddings(model_name = model_id)
        log_info(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {model_id}", logger)
        return embeddings
    except Exception as e:
        log_error(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {model_id} - é”™è¯¯: {str(e)}", logger)
        raise

# ä»æœ¬åœ°çš„ollama åŠ è½½ embeddings æ¨¡å‹
def load_embeddings_model_from_ollama(model_id):
    """ä»æœ¬åœ°åŠ è½½åµŒå…¥æ¨¡å‹"""
    log_info(f"ğŸ“¦ åŠ è½½åµŒå…¥æ¨¡å‹: {model_id}", logger)

    try:
        embeddings = OllamaEmbeddings(
            model = model_id,
            base_url = "http://localhost:11434"  # é»˜è®¤å¯çœç•¥
        )
        log_info(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {model_id}", logger)
        return embeddings
    except Exception as e:
        log_error(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {model_id} - é”™è¯¯: {str(e)}", logger)
        raise

# ä»æœ¬åœ°çš„ollama åŠ è½½ llm æ¨¡å‹
def load_llm_from_ollama(model_id):
    """ä»æœ¬åœ°åŠ è½½ llm æ¨¡å‹"""
    try:
        llm = OllamaLLM(
            model = model_id,
            base_url = "http://localhost:11434"  # é»˜è®¤å¯çœç•¥
        )
        log_info(f"âœ… llm æ¨¡å‹åŠ è½½æˆåŠŸ: {model_id}", logger)
        return llm
    except Exception as e:
        log_error(f"llm æ¨¡å‹åŠ è½½å¤±è´¥: {model_id} - é”™è¯¯: {str(e)}", logger)
        raise   

# åŠ è½½æœ¬åœ° LLMï¼Œåœ¨mac M1èŠ¯ç‰‡ä¸Šä½¿ç”¨ MPS åŠ é€Ÿ
def load_llm_from_local(model_path):
    """åŠ è½½æœ¬åœ°LLMï¼Œæ”¯æŒç¼“å­˜é¿å…é‡å¤åŠ è½½"""
    log_model_loading_start(model_path, logger)
    if not os.path.exists(model_path):
        log_error(f"æ¨¡å‹ä¸å­˜åœ¨: {model_path}", logger)
        raise FileNotFoundError(f"æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
    else:
        # è®¾å¤‡æ£€æµ‹
        if torch.backends.mps.is_available():
            device = "mps"
            data_type = torch.float16
        elif torch.cuda.is_available():
            device = "cuda"
            data_type = torch.float16
        else:
            device = "cpu"
            data_type = torch.float32
            
    log_device_detection(device, str(data_type), logger)

    # åŠ è½½åˆ†è¯å™¨tokenizer å’Œæ¨¡å‹ model
    try:
        # åŠ è½½ tokenizer
        log_info(f"ğŸ”¤ åŠ è½½åˆ†è¯å™¨: {model_path}", logger)
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code = True
        )

        # åŠ è½½æ¨¡å‹
        log_info(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_path}", logger)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map = device, # "auto"
            trust_remote_code = True,
            dtype = data_type,
        )

        # è¿”å› tokenizer å’Œ model
        return tokenizer, model
        
    except Exception as e:
        log_model_loading_failed(model_path, str(e), logger)
        raise

# æµ‹è¯•LLMæ¨¡å‹åŠŸèƒ½, ç›´æ¥ä½¿ç”¨tokenizer å’Œ model, ä½¿ç”¨generate æ–¹æ³•
def run_local_llm(tokenizer, model, messages):
    """æµ‹è¯•LLMæ¨¡å‹åŠŸèƒ½"""
    log_info("ğŸ§ª å¼€å§‹LLMæ¨¡å‹åŠŸèƒ½æµ‹è¯•", logger)

    # Promptç”Ÿæˆï¼šä½¿ç”¨ tokenizer å†…ç½®æ¨¡æ¿ç”Ÿæˆ promptï¼ˆå…³é”®ï¼ï¼‰
    # ä½¿ç”¨ tokenizer.apply_chat_template()ï¼Œæ§åˆ¶è¾“å…¥æ ¼å¼ï¼Œç¡®ä¿ç¬¦åˆå®˜æ–¹è§„èŒƒ
    text = tokenizer.apply_chat_template(
        messages,
        tokenize = False,
        add_generation_prompt = True  # æ·»åŠ  <|im_start|>assistant æ ‡è®°
    )

    # ç¼–ç ä¸º input_ids
    model_inputs = tokenizer([text], return_tensors = "pt").to(model.device)

    # ç”Ÿæˆå›å¤
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=256,
        do_sample=True,
        temperature=0.9,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id
    )

    # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥ promptï¼‰
    generated_ids = generated_ids[:, model_inputs.input_ids.shape[1]:]

    # è§£ç 
    response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    log_info(f"ğŸ’¬ æ¨¡å‹å›ç­”: {response.strip()}", logger)
    log_info("âœ… LLMæ¨¡å‹åŠŸèƒ½æµ‹è¯•å®Œæˆ", logger)
    return response

def run_ollama_llm(ollama_llm, prompt):
    """æµ‹è¯• Ollama LLM æ¨¡å‹åŠŸèƒ½"""
    log_info("ğŸ§ª å¼€å§‹ Ollama LLM æ¨¡å‹åŠŸèƒ½æµ‹è¯•", logger)
    
    try:
        # æ–¹å¼ä¸€ï¼šä½¿ç”¨ invoke æ–¹æ³•ï¼ˆæ¨èï¼‰
        log_info(f"ğŸ“ å‘é€æç¤º: {prompt}", logger)
        response = ollama_llm.invoke(prompt)
        log_info(f"ğŸ’¬ æ¨¡å‹å›ç­”: {response.strip()}", logger)
        
        # æ–¹å¼äºŒï¼šä½¿ç”¨ generate æ–¹æ³•ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
        '''
        results = ollama_llm.generate([prompt])
        response = results.generations[0][0].text
        log_info(f"ğŸ’¬ æ¨¡å‹å›ç­”: {response.strip()}", logger)
        '''
        
        log_info("âœ… Ollama LLM æ¨¡å‹åŠŸèƒ½æµ‹è¯•å®Œæˆ", logger)
        return response
        
    except Exception as e:
        log_error(f"Ollama æµ‹è¯•å¤±è´¥: {str(e)}", logger)
        raise

if __name__ == "__main__":
    MODEL_REPO = 2 # 1 for huggingface_hub, 2 for modelscope repo
    MODEL_DIR = "/Users/carlos/Desktop/PileGo.Ai/ahum_llm/llms"
    LLM_MODEL_LOCAL_TAG = "Qwen/Qwen3-0.6B"
    EMBEDDING_MODEL_LOCAL_TAG = "Qwen/Qwen3-Embedding-0.6B"
    EMBEDDING_MODEL_OLLAMA_TAG = "bge-m3"
    LLM_MODEL_OLLAMA_TAG = "qwen3:8b"

    # ä¸‹è½½embedding model to local
    embedding_model_dir = download_model_to_local(EMBEDDING_MODEL_LOCAL_TAG, MODEL_DIR, MODEL_REPO)
    print(f"Embedding model downloaded to: {embedding_model_dir}")
    ################################################
    ################ local model ###################
    ################################################
    
    '''# æ„é€ å¯¹è¯æ¶ˆæ¯ï¼ˆQwen3 æ ‡å‡†æ ¼å¼ï¼‰
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "å—¨ï¼æˆ‘æ˜¯è°ï¼Ÿ"}
    ]
    prompt = "ä½ å¥½ï¼è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"

    llm = PI_LLM(isLocal=True)
    local_model_dir = llm.download_model(LLM_MODEL_LOCAL_TAG, MODEL_DIR, MODEL_REPO)
    llm.load_llm_model(local_model_dir)
    # print(llm.tokenizer, llm.llm_model)
    run_local_llm(llm.tokenizer, llm.llm_model, messages)'''

    ################################################
    ################ ollama model ##################
    ################################################
    
    # Ollama ä½¿ç”¨ç®€å•çš„æ–‡æœ¬æç¤ºï¼Œä¸éœ€è¦å¤æ‚çš„ tokenizer å¤„ç†
    '''
    prompt = "ä½ å¥½ï¼è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
    
    llm = PI_LLM(isLocal=False)
    llm.load_llm_model(LLM_MODEL_OLLAMA_TAG)
    print(llm.ollama_llm)
    llm.load_embeddings_model(EMBEDDING_MODEL_OLLAMA_TAG)
    print(llm.embeddings)
    # print(local_model_dir)
    # print(llm.tokenizer, llm.model)
    
     # æµ‹è¯• Ollama æ¨¡å‹
    response = run_ollama_llm(llm.ollama_llm, prompt)
    print(f"æœ€ç»ˆå“åº”: {response}")
    '''