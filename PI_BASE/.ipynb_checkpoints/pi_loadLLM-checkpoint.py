from modelscope import snapshot_download as ms_snapshot_download
from modelscope.hub.constants import MODEL_ID_SEPARATOR
# from huggingface_hub import snapshot_download, hf_hub_download
from huggingface_hub import snapshot_download as hf_snapshot_download 

from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os

# å¯¼å…¥PIé¡¹ç›®æ—¥å¿—æ¨¡å—
from pi_log import *

# å…¨å±€å˜é‡å­˜å‚¨å·²åŠ è½½çš„æ¨¡å‹
_LOADED_MODELS = {}

# è·å–loggerå®ä¾‹ï¼ˆå¯ç”¨æ–‡ä»¶æ—¥å¿—ï¼‰
logger = get_logger("PI_LLM", log_to_file=True)

# ä¸‹è½½æ•´ä¸ªæ¨¡å‹ä»“åº“åˆ°æœ¬åœ°ï¼Œ è¿™é‡Œä½¿ç”¨modelscope çš„snapshot_download æ–¹æ³•
def download_model_to_local(model_id, model_dir):
    """ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç›®å½•"""
    model_path = os.path.join(model_dir, model_id.replace(MODEL_ID_SEPARATOR, "/"))
    
    log_download_start(model_id, model_dir, logger)
    
    if os.path.exists(model_path):
        log_file_exists(model_path, logger)
        return model_path
    else:
        try:
            local_dir = ms_snapshot_download(
                model_id = model_id,
                revision = "master",
                cache_dir = model_dir,
                max_workers = 4
            )
            log_download_complete(local_dir, logger)
            return local_dir
        except Exception as e:
            log_error(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {model_id} - é”™è¯¯: {str(e)}", logger)
            raise

def load_embedding_model_from_local(model_id):
    """ä»æœ¬åœ°åŠ è½½åµŒå…¥æ¨¡å‹"""
    log_info(f"ğŸ“¦ åŠ è½½åµŒå…¥æ¨¡å‹: {model_id}", logger)
    try:
        model = SentenceTransformer(model_id)
        log_info(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {model_id}", logger)
        return model
    except Exception as e:
        log_error(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {model_id} - é”™è¯¯: {str(e)}", logger)
        raise

# åŠ è½½æœ¬åœ° LLMï¼Œåœ¨mac M1èŠ¯ç‰‡ä¸Šä½¿ç”¨ MPS åŠ é€Ÿ
def load_llm_from_local(model_path, use_cache=True):
    """åŠ è½½æœ¬åœ°LLMï¼Œæ”¯æŒç¼“å­˜é¿å…é‡å¤åŠ è½½"""
    log_model_loading_start(model_path, logger)
    
    # æ£€æŸ¥ç¼“å­˜
    if use_cache and model_path in _LOADED_MODELS:
        log_cache_hit(model_path, logger)
        return _LOADED_MODELS[model_path]
    else:
        log_cache_miss(model_path, logger)
        
        # è®¾å¤‡æ£€æµ‹
        if torch.backends.mps.is_available():
            device = "mps"
            torch_dtype = torch.float16
        elif torch.cuda.is_available():
            device = "cuda"
            torch_dtype = torch.float16
        else:
            device = "cpu"
            torch_dtype = torch.float32
            
        log_device_detection(device, str(torch_dtype), logger)

    try:
        # åŠ è½½ tokenizer
        log_info(f"ğŸ”¤ åŠ è½½åˆ†è¯å™¨: {model_path}", logger)
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True
        )

        # åŠ è½½æ¨¡å‹
        log_info(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_path}", logger)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16,
        )

        # ç¼“å­˜æ¨¡å‹
        if use_cache:
            _LOADED_MODELS[model_path] = (tokenizer, model)
            log_cache_store(model_path, logger)

        log_model_loading_success(model_path, device, logger)
        return tokenizer, model
        
    except Exception as e:
        log_model_loading_failed(model_path, str(e), logger)
        raise

def test_embedding_model(model):
    """æµ‹è¯•åµŒå…¥æ¨¡å‹åŠŸèƒ½"""
    log_info("ğŸ§ª æµ‹è¯•åµŒå…¥æ¨¡å‹åŠŸèƒ½", logger)
    sentences = ["Hello, world!", "ä½ å¥½ï¼Œä¸–ç•Œï¼"]
    embeddings = model.encode(sentences)
    log_info(f"ğŸ“Š Embedding shape: {embeddings.shape}", logger)
    log_info(f"ğŸ“ˆ First 5 dims of first sentence: {embeddings[0][:5]}", logger)
    return embeddings

def test_llm(tokenizer, model):
    """æµ‹è¯•LLMæ¨¡å‹åŠŸèƒ½"""
    log_info("ğŸ§ª å¼€å§‹LLMæ¨¡å‹åŠŸèƒ½æµ‹è¯•", logger)
    
    # æ„é€ å¯¹è¯æ¶ˆæ¯ï¼ˆQwen3 æ ‡å‡†æ ¼å¼ï¼‰
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "å—¨ï¼æˆ‘æ˜¯è°ï¼Ÿ"}
    ]

    # ä½¿ç”¨ tokenizer å†…ç½®æ¨¡æ¿ç”Ÿæˆ promptï¼ˆå…³é”®ï¼ï¼‰
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True  # æ·»åŠ  <|im_start|>assistant æ ‡è®°
    )

    # ç¼–ç ä¸º input_ids
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

    # ç”Ÿæˆå›å¤
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=256,
        do_sample=True,
        temperature=0.9,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id
    )

    # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥ promptï¼‰
    generated_ids = generated_ids[:, model_inputs.input_ids.shape[1]:]

    # è§£ç 
    response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    log_info(f"ğŸ’¬ æ¨¡å‹å›ç­”: {response.strip()}", logger)
    log_info("âœ… LLMæ¨¡å‹åŠŸèƒ½æµ‹è¯•å®Œæˆ", logger)

def clear_model_cache(model_path=None):
    """æ¸…ç†æ¨¡å‹ç¼“å­˜"""
    if model_path is None:
        # æ¸…ç†æ‰€æœ‰ç¼“å­˜
        _LOADED_MODELS.clear()
        log_cache_clear("ALL", logger)
    elif model_path in _LOADED_MODELS:
        # æ¸…ç†æŒ‡å®šæ¨¡å‹
        del _LOADED_MODELS[model_path]
        log_cache_clear(model_path, logger)
    else:
        log_warning(f"æœªæ‰¾åˆ°ç¼“å­˜: {model_path}", logger)

# ä½¿ç”¨ç¤ºä¾‹
# clear_model_cache()  # æ¸…ç†å…¨éƒ¨
# clear_model_cache("/path/to/specific/model")  # æ¸…ç†ç‰¹å®šæ¨¡å‹

if __name__ == "__main__":
    '''
    # è®¾ç½®ç½‘ç»œç›¸å…³ç¯å¢ƒå˜é‡
    import os
    os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'
    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'  # ä½¿ç”¨é•œåƒç«™ç‚¹
    '''

    model_dir = "/Users/carlos/Desktop/PileGo.Ai/ahum_llm/llms"
    
    embeddings_model_tag = "BAAI/bge-m3"
    llm_model_tag      = "Qwen/Qwen3-1.7B"
    llm_model_path = os.path.join(model_dir, llm_model_tag.replace(MODEL_ID_SEPARATOR, "/"))
    # embeddings_model_path = os.path.join(model_dir, embeddings_model_tag.replace(MODEL_ID_SEPARATOR, "/"))

    download_model_to_local(llm_model_tag, model_dir)
    llm_model = load_llm_from_local(llm_model_path)
   
    # embeddings_model = load_embedding_model_from_local(embeddings_model_path)
    # test_embedding_model(embeddings_model)
    test_llm(llm_model[0], llm_model[1])

    '''
    # åˆ¤æ–­æ˜¯å¦åŠ è½½æˆåŠŸ
    if is_model_loaded(llm_model[0], llm_model[1]):
        print("âœ… Qwen3-1.7B æ¨¡å‹å·²æˆåŠŸåŠ è½½ï¼")
        # æ‰§è¡Œæ¨ç†...
    else:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„æˆ–ç¯å¢ƒ")
        exit(1)'''