from pypdf import PdfReader
# from langchain.chains.question_answering import load_qa_chain
# from langchain_openai import OpenAI
from langchain_community.callbacks.manager import get_openai_callback
from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.vectorstores import FAISS
# from langchain.retrievers import MultiQueryRetriever
from langchain_classic.retrievers.multi_query import MultiQueryRetriever
# from langchain.retrievers.multi_query import MultiQueryRetriever
# from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_community.llms import Tongyi
from typing import List, Tuple
import os
import pickle
from langchain_core.callbacks.manager import CallbackManagerForRetrieverRun

## å¯¼å…¥ pi_down_load_models ä¸­çš„ PI_LLMï¼Œç»´æŠ¤ä¸€ä¸ªclass, åŒ…å«äº† llm å’Œembedding æ¨¡å‹
from pi_down_load_models import PiLLM
ollama_EMBEDDING_MODEL = "bge-m3"
ollama_LLM_MODEL = "qwen3:8b"

MODEL_DIR = "/Users/carlos/Desktop/PileGo.Ai/ahum_llm/llms"
LLM_MODEL_LOCAL_TAG = "Qwen/Qwen3-0.6B"
EMBEDDING_MODEL_LOCAL_TAG = "Qwen/Qwen3-Embedding-0.6B"

# åˆå§‹åŒ– PiLLMï¼šåŠ è½½ LLM å’Œ Embedding æ¨¡å‹
cLLM = PiLLM()
cLLM.load_embeddings_model(ollama_EMBEDDING_MODEL, isLocal=False)
cLLM.load_llm_model(ollama_LLM_MODEL, isLocal=False)
llm = cLLM.llm_model
embeddings = cLLM.embeddings
if llm is None:
    raise RuntimeError("LLM æ¨¡å‹æœªæˆåŠŸåŠ è½½ï¼è¯·æ£€æŸ¥ PiLLM.load_llm_model çš„å®ç°å’Œæ¨¡å‹åç§°ã€‚")
if embeddings is None:
    raise RuntimeError("åµŒå…¥æ¨¡å‹æœªæˆåŠŸåŠ è½½ï¼è¯·æ£€æŸ¥ PiLLM.load_embeddings_model çš„å®ç°å’Œæ¨¡å‹åç§°ã€‚")

# è·å–ç¯å¢ƒå˜é‡ä¸­çš„ DASHSCOPE_API_KEY
DASHSCOPE_API_KEY = os.getenv('DASHSCOPE_API_KEY')
if not DASHSCOPE_API_KEY:
    raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")

def extract_text_with_page_numbers(pdf) -> Tuple[str, List[int]]:
    """
    ä»PDFä¸­æå–æ–‡æœ¬å¹¶è®°å½•æ¯è¡Œæ–‡æœ¬å¯¹åº”çš„é¡µç 
    
    å‚æ•°:
        pdf: PDFæ–‡ä»¶å¯¹è±¡
    
    è¿”å›:
        text: æå–çš„æ–‡æœ¬å†…å®¹
        page_numbers: æ¯è¡Œæ–‡æœ¬å¯¹åº”çš„é¡µç åˆ—è¡¨
    """
    text = ""
    page_numbers = []

    for page_number, page in enumerate(pdf.pages, start=1):
        extracted_text = page.extract_text()
        if extracted_text:
            text += extracted_text
            page_numbers.extend([page_number] * len(extracted_text.split("\n")))
        else:
            print(f"No text found on page {page_number}.")

    return text, page_numbers

def process_text_with_splitter(text: str, page_numbers: List[int], save_path: str = None) -> FAISS:
    """
    å¤„ç†æ–‡æœ¬å¹¶åˆ›å»ºå‘é‡å­˜å‚¨
    
    å‚æ•°:
        text: æå–çš„æ–‡æœ¬å†…å®¹
        page_numbers: æ¯è¡Œæ–‡æœ¬å¯¹åº”çš„é¡µç åˆ—è¡¨
        save_path: å¯é€‰ï¼Œä¿å­˜å‘é‡æ•°æ®åº“çš„è·¯å¾„
    
    è¿”å›:
        knowledgeBase: åŸºäºFAISSçš„å‘é‡å­˜å‚¨å¯¹è±¡
    """
    # åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨ï¼Œç”¨äºå°†é•¿æ–‡æœ¬åˆ†å‰²æˆå°å—
    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", ".", " ", ""],
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )

    # åˆ†å‰²æ–‡æœ¬
    chunks = text_splitter.split_text(text)
    print(f"æ–‡æœ¬è¢«åˆ†å‰²æˆ {len(chunks)} ä¸ªå—ã€‚")

    embeddings = cLLM.embeddings
    
    # ä»æ–‡æœ¬å—åˆ›å»ºçŸ¥è¯†åº“
    knowledgeBase = FAISS.from_texts(chunks, embeddings)
    print("å·²ä»æ–‡æœ¬å—åˆ›å»ºçŸ¥è¯†åº“ã€‚")
    
    # æ”¹è¿›ï¼šå­˜å‚¨æ¯ä¸ªæ–‡æœ¬å—å¯¹åº”çš„é¡µç ä¿¡æ¯
    lines = text.split("\n")
    page_info = {}
    for chunk in chunks:
        # æŸ¥æ‰¾chunkåœ¨åŸå§‹æ–‡æœ¬ä¸­çš„å¼€å§‹ä½ç½®
        start_idx = text.find(chunk[:100])  # ä½¿ç”¨chunkçš„å‰100ä¸ªå­—ç¬¦ä½œä¸ºå®šä½ç‚¹
        if start_idx == -1:
            # å¦‚æœæ‰¾ä¸åˆ°ç²¾ç¡®åŒ¹é…ï¼Œåˆ™ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…
            for i, line in enumerate(lines):
                if chunk.startswith(line[:min(50, len(line))]):
                    start_idx = i
                    break
            if start_idx == -1:
                for i, line in enumerate(lines):
                    if line and line in chunk:
                        start_idx = text.find(line)
                        break
        if start_idx != -1:
            line_count = text[:start_idx].count("\n")
            if line_count < len(page_numbers):
                page_info[chunk] = page_numbers[line_count]
            else:
                page_info[chunk] = page_numbers[-1] if page_numbers else 1
        else:
            page_info[chunk] = -1
    knowledgeBase.page_info = page_info
    
    # å¦‚æœæä¾›äº†ä¿å­˜è·¯å¾„ï¼Œåˆ™ä¿å­˜å‘é‡æ•°æ®åº“å’Œé¡µç ä¿¡æ¯
    if save_path:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(save_path, exist_ok=True)
        
        # ä¿å­˜FAISSå‘é‡æ•°æ®åº“
        knowledgeBase.save_local(save_path)
        print(f"å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°: {save_path}")
        
        # ä¿å­˜é¡µç ä¿¡æ¯åˆ°åŒä¸€ç›®å½•
        with open(os.path.join(save_path, "page_info.pkl"), "wb") as f:
            pickle.dump(page_info, f)
        print(f"é¡µç ä¿¡æ¯å·²ä¿å­˜åˆ°: {os.path.join(save_path, 'page_info.pkl')}")

    return knowledgeBase

def load_knowledge_base(load_path: str, embeddings = None) -> FAISS:
    """
    ä»ç£ç›˜åŠ è½½å‘é‡æ•°æ®åº“å’Œé¡µç ä¿¡æ¯
    
    å‚æ•°:
        load_path: å‘é‡æ•°æ®åº“çš„ä¿å­˜è·¯å¾„
        embeddings: å¯é€‰ï¼ŒåµŒå…¥æ¨¡å‹ã€‚å¦‚æœä¸ºNoneï¼Œå°†åˆ›å»ºä¸€ä¸ªæ–°çš„DashScopeEmbeddingså®ä¾‹
    
    è¿”å›:
        knowledgeBase: åŠ è½½çš„FAISSå‘é‡æ•°æ®åº“å¯¹è±¡
    """
    # å¦‚æœæ²¡æœ‰æä¾›åµŒå…¥æ¨¡å‹ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„
    if embeddings is None:
        embeddings = DashScopeEmbeddings(
            model="text-embedding-v1",
            dashscope_api_key=DASHSCOPE_API_KEY,
        )
    
    # åŠ è½½FAISSå‘é‡æ•°æ®åº“ï¼Œæ·»åŠ allow_dangerous_deserialization=Trueå‚æ•°ä»¥å…è®¸ååºåˆ—åŒ–
    knowledgeBase = FAISS.load_local(load_path, embeddings, allow_dangerous_deserialization=True)
    print(f"å‘é‡æ•°æ®åº“å·²ä» {load_path} åŠ è½½ã€‚")
    
    # åŠ è½½é¡µç ä¿¡æ¯
    page_info_path = os.path.join(load_path, "page_info.pkl")
    if os.path.exists(page_info_path):
        with open(page_info_path, "rb") as f:
            page_info = pickle.load(f)
        knowledgeBase.page_info = page_info
        print("é¡µç ä¿¡æ¯å·²åŠ è½½ã€‚")
    else:
        print("è­¦å‘Š: æœªæ‰¾åˆ°é¡µç ä¿¡æ¯æ–‡ä»¶ã€‚")
    
    return knowledgeBase


###### ä½¿ç”¨MultiQueryRetriever åŸºæœ¬å·¥ä½œæµç¨‹ï¼š######
###### 1. ç”¨æˆ·è¾“å…¥å•ä¸ªæŸ¥è¯¢
###### 2. LLM ç”Ÿæˆå¤šä¸ªç›¸å…³æŸ¥è¯¢å˜ä½“, è¿™é‡Œä½¿ç”¨MultiQueryRetriever
###### 3. å¯¹æ¯ä¸ªæŸ¥è¯¢å˜ä½“æ‰§è¡Œæ£€ç´¢
###### 4. åˆå¹¶å¹¶å»é‡æ‰€æœ‰æ£€ç´¢ç»“æœ
###### 5. è¿”å›æœ€ç›¸å…³çš„ç»“æœ
################################################

############ åˆ›å»ºMultiQueryRetriever ############
## è¿”å›MultiQueryRetriever æ£€ç´¢å™¨
# åˆ›å»ºMultiQueryRetriever
def create_multi_query_retriever(vectorstore, llm):
    """
    åˆ›å»ºMultiQueryRetriever
    
    å‚æ•°:
        vectorstore: å‘é‡æ•°æ®åº“
        llm: å¤§è¯­è¨€æ¨¡å‹ï¼Œç”¨äºæŸ¥è¯¢æ”¹å†™
    
    è¿”å›:
        retriever: MultiQueryRetrieverå¯¹è±¡
    """

    # åˆ›å»ºåŸºç¡€æ£€ç´¢å™¨ï¼šè¿™ä¸ªåŸºç¡€æ£€ç´¢å™¨æ˜¯ç”±å‘é‡æ•°æ®åº“è½¬åŒ–æˆçš„ã€‚
    base_retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
    
    # åˆ›å»ºMultiQueryRetriever
    retriever = MultiQueryRetriever.from_llm(
        retriever=base_retriever,
        llm=llm,
        prompt = QUERY_PROMPT
    )
    
    return retriever

# å¦‚æœä½ æƒ³è¦ç±»ä¼¼ MultiQuery çš„æ•ˆæœï¼Œå¯ä»¥æ‰‹åŠ¨å®ç°ï¼š
def enhanced_retrieval(query, base_retriever, llm):
    """æ‰‹åŠ¨å®ç°å¢å¼ºæ£€ç´¢"""
    # 1. ä½¿ç”¨åŸºç¡€æ£€ç´¢
    docs = base_retriever.get_relevant_documents(query)
    
    # 2. å¯é€‰ï¼šä½¿ç”¨ LLM ç”Ÿæˆç›¸å…³æŸ¥è¯¢è¿›è¡Œé¢å¤–æ£€ç´¢
    # additional_queries = llm.invoke(f"Related queries for: {query}")
    # for q in additional_queries:
    #     docs.extend(base_retriever.get_relevant_documents(q))
    
    return docs

# ä½¿ç”¨MultiQueryRetrieverå¤„ç†æŸ¥è¯¢
def process_query_with_multi_retriever(query: str, retriever, llm, knowledgeBase):
    """
    ä½¿ç”¨MultiQueryRetrieverå¤„ç†æŸ¥è¯¢
    
    å‚æ•°:
        query: ç”¨æˆ·æŸ¥è¯¢
        retriever: MultiQueryRetrieverå¯¹è±¡
        llm: å¤§è¯­è¨€æ¨¡å‹
    
    è¿”å›:
        response: å›ç­”
        unique_pages: ç›¸å…³æ–‡æ¡£çš„é¡µç é›†åˆ
    """
    # æ‰§è¡ŒæŸ¥è¯¢ï¼Œè·å–ç›¸å…³æ–‡æ¡£
    docs = retriever.invoke(query)
    print(f"æ‰¾åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£")

    # æ„å»º RAG é“¾ï¼ˆç°ä»£å†™æ³•ï¼‰
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    qa_prompt = PromptTemplate.from_template(
        "ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\n{context}\n\né—®é¢˜ï¼š{question}\nç­”æ¡ˆï¼š"
    )

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    response_text = rag_chain.invoke(query)

    # è·å–é¡µç 
    unique_pages = set()
    for doc in docs:
        page = knowledgeBase.page_info.get(doc.page_content.strip(), "æœªçŸ¥")
        unique_pages.add(page)

    return {"output_text": response_text}, unique_pages

## from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

def main():
    pdf_path = './æµ¦å‘ä¸Šæµ·æµ¦ä¸œå‘å±•é“¶è¡Œè¥¿å®‰åˆ†è¡Œä¸ªé‡‘å®¢æˆ·ç»ç†è€ƒæ ¸åŠæ³•.pdf'
    vector_db_path = './vector_db'

    # åŠ è½½æˆ–åˆ›å»ºå‘é‡æ•°æ®åº“
    if os.path.exists(vector_db_path) and os.path.isdir(vector_db_path):
        print(f"å‘ç°ç°æœ‰å‘é‡æ•°æ®åº“: {vector_db_path}")
        knowledgeBase = load_knowledge_base(vector_db_path, embeddings)
    else:
        print("æœªæ‰¾åˆ°å‘é‡æ•°æ®åº“ï¼Œæ­£åœ¨ä» PDF åˆ›å»º...")
        pdf_reader = PdfReader(pdf_path)
        text, page_numbers = extract_text_with_page_numbers(pdf_reader)
        print(f"æå–æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        knowledgeBase = process_text_with_splitter(text, page_numbers, save_path=vector_db_path)

    # === Step 1: åˆ›å»º MultiQueryRetrieverï¼ˆä½¿ç”¨ä¸“ç”¨ promptï¼‰===
    QUERY_PROMPT = PromptTemplate(
        input_variables=["question"],
        template=(
            "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„é—®é¢˜æ”¹å†™æˆ3ä¸ªè¯­ä¹‰ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„æœç´¢æŸ¥è¯¢ã€‚\n"
            "è¿™äº›æŸ¥è¯¢å°†ç”¨äºæ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚è¯·æ¯è¡Œè¾“å‡ºä¸€ä¸ªæŸ¥è¯¢ï¼Œä¸è¦ç¼–å·ï¼Œä¸è¦è§£é‡Šã€‚\n\n"
            "åŸå§‹é—®é¢˜: {question}\n\n"
            "æ”¹å†™åçš„æŸ¥è¯¢ï¼š"
        ),
    )

    base_retriever = knowledgeBase.as_retriever(search_kwargs={"k": 4})
    multi_query_retriever = MultiQueryRetriever.from_llm(
        retriever=base_retriever,
        llm=llm,
        prompt=QUERY_PROMPT
    )

    # === Step 2: å®šä¹‰ RAG å›ç­”é“¾ï¼ˆä½¿ç”¨ QA promptï¼‰===
    QA_PROMPT = PromptTemplate.from_template(
        "ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸çŸ¥é“ï¼Œè¯·å›ç­”â€œæ ¹æ®æä¾›çš„èµ„æ–™æ— æ³•ç¡®å®šâ€ã€‚\n\n"
        "ä¸Šä¸‹æ–‡:\n{context}\n\n"
        "é—®é¢˜: {question}\n"
        "ç­”æ¡ˆ:"
    )

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
        {
            "context": multi_query_retriever | format_docs,
            "question": RunnablePassthrough()
        }
        | QA_PROMPT
        | llm
        | StrOutputParser()
    )

    # === Step 3: æ‰§è¡ŒæŸ¥è¯¢ ===
    queries = [
        "å®¢æˆ·ç»ç†è¢«æŠ•è¯‰äº†ï¼ŒæŠ•è¯‰ä¸€æ¬¡æ‰£å¤šå°‘åˆ†",
        "å®¢æˆ·ç»ç†æ¯å¹´è¯„è˜ç”³æŠ¥æ—¶é—´æ˜¯æ€æ ·çš„ï¼Ÿ",
        "å®¢æˆ·ç»ç†çš„è€ƒæ ¸æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]

    for query in queries:
        print("\n" + "=" * 60)
        print(f"ğŸ” æŸ¥è¯¢: {query}")

        # è·å–æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ˆç”¨äºé¡µç ï¼‰
        retrieved_docs = multi_query_retriever.invoke(query)
        print(f"ğŸ“„ æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³ç‰‡æ®µ")

        # è·å–å›ç­”
        answer = rag_chain.invoke(query)

        # æå–å”¯ä¸€æ¥æºé¡µç 
        unique_pages = set()
        for doc in retrieved_docs:
            content = doc.page_content.strip()
            page = knowledgeBase.page_info.get(content, "æœªçŸ¥")
            unique_pages.add(page)

        # è¾“å‡ºç»“æœ
        print("\nğŸ’¡ å›ç­”:")
        print(answer)
        print("\nğŸ“š æ¥æºé¡µç :")
        for p in sorted(unique_pages):
            print(f"  - ç¬¬ {p} é¡µ")
        print("=" * 60)

if __name__ == "__main__":
    main()

